{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from pathlib import Path\n",
    "from transformers import GPT2Tokenizer, GPT2Model,  AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "project_dir = Path().cwd().parent\n",
    "data_dir = project_dir.joinpath(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/wonseok/.cache/huggingface/datasets/trivia_qa/unfiltered/1.2.0/ee76d8a9403e71177e2a3fa7e414d1ee28a79a0970d9176f62f268798aa64b31)\n",
      "Found cached dataset trivia_qa (/home/wonseok/.cache/huggingface/datasets/trivia_qa/unfiltered/1.2.0/ee76d8a9403e71177e2a3fa7e414d1ee28a79a0970d9176f62f268798aa64b31)\n"
     ]
    }
   ],
   "source": [
    "# fist start with trivia\n",
    "# datasets.config.DOWNLOADED_DATASETS_PATH = \"/home/public/wjang/data/triviaqa-wiki\"\n",
    "# datasets.config.DOWNLOADED_DATASETS_PATH = data_dir\n",
    "\n",
    "train = load_dataset('trivia_qa', 'unfiltered', split=\"train[:8000]\")\n",
    "test = load_dataset('trivia_qa', 'unfiltered', split='train[8000:10000]')\n",
    "# test = load_dataset('trivia_qa', 'unfiltered', split ='test[:200]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wonseok/Insync/gememy85@gmail.com/Google Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb ì…€ 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# save files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train\u001b[39m.\u001b[39mto_json(data_dir\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39mraw/triviaqa_raw_train.json\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test\u001b[39m.\u001b[39mto_json(data_dir\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39mraw/triviaqa_raw_test.json\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# save files\n",
    "\n",
    "train.to_json(data_dir.joinpath('raw/triviaqa_raw_train.json'))\n",
    "test.to_json(data_dir.joinpath('raw/triviaqa_raw_test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/wonseok/.cache/huggingface/datasets/json/default-62b5c3c6749fb4c0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Found cached dataset json (/home/wonseok/.cache/huggingface/datasets/json/default-e502800c41ae9e81/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset('json', data_files=data_dir.joinpath('raw/triviaqa_raw_train.json').as_posix(), split='train')\n",
    "test = load_dataset('json', data_files=data_dir.joinpath('raw/triviaqa_raw_test.json').as_posix(), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [source['wiki_context'][0] if len(source['wiki_context']) > 0 else 'no context'for source in train['entity_pages'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "bos = '<|bos|>'\n",
    "eos = '<|eos|>'\n",
    "pad = '<|pad|>'\n",
    "sep = '<|sep|>'\n",
    "\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad, 'sep_token': sep}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=bos + \" $A \" + eos,\n",
    "    special_tokens=[(eos, tokenizer.eos_token_id), (bos, tokenizer.bos_token_id)],\n",
    ")\n",
    "\n",
    "# need to resize model vocabulary\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context[0:2], return_offsets_mapping=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[6435, 37555, 318, 257, 11150, 3474, 4445, 290, 3502, 1605, 9048, 10283, 3194, 290, 18542, 416, 7516, 337, 13, 3059, 37314, 11, 543, 4966, 422, 3267, 362, 11, 11445, 11, 284, 3945, 1511, 11, 4751, 11, 8282, 287, 302, 48381, 20875, 13, 383, 10283, 318, 262, 749, 2968, 290, 14212, 287, 262, 2106, 286, 9048, 22670, 11, 351, 1596, 11, 4531, 22, 22670, 3199, 287, 477, 11, 1642, 340, 366, 853, 14632, 262, 14069, 1621, 1683, 1297, 416, 530, 1692, 852, 1911, 220, 1629, 663, 9103, 11, 2631, 37555, 4966, 287, 625, 362, 11, 8054, 14741, 11, 351, 257, 7183, 1056, 286, 36561, 1510, 287, 5441, 2678, 11, 290, 373, 14251, 656, 2310, 8950, 13, 220, 632, 4193, 284, 20534, 262, 1440, 12, 35330, 32261, 10283, 355, 262, 3210, 287, 262, 1578, 1829, 11, 220, 290, 1978, 351, 663, 20813, 7366, 3059, 37314, 517, 621, 720, 16, 2997, 13, 1432, 22272, 82, 286, 262, 10283, 389, 991, 11150, 3474, 290, 1057, 287, 2048, 790, 471, 13, 50, 13, 7533, 13, 198, 198, 464, 10283, 13692, 5000, 319, 257, 28685, 3592, 286, 1862, 1751, 11, 351, 645, 3402, 4044, 3435, 13, 383, 1388, 2095, 11, 11526, 4373, 11, 318, 502, 988, 11, 10927, 11, 290, 16523, 2116, 12, 39745, 13, 679, 318, 5906, 284, 6129, 257, 479, 578, 11, 1592, 257, 9283, 983, 11, 393, 4829, 257, 4346, 13, 220, 198, 198, 6435, 37555, 318, 530, 286, 262, 4187, 378, 22670, 351, 17580, 11, 10590, 11, 290, 1307, 15071, 625, 36257, 326, 45671, 287, 262, 11445, 82, 13, 220, 383, 10283, 338, 14733, 357, 265, 1551, 1141, 663, 705, 1899, 82, 9103, 8, 318, 39705, 3716, 11, 290, 262, 3435, 6, 12213, 7042, 257, 256, 9248, 286, 6958, 326, 10357, 262, 10283, 13, 198, 198, 6435, 37555, 8793, 11091, 1943, 351, 663, 5581, 38102, 11, 1811, 286, 543, 11, 1390, 317, 11526, 4373, 6786, 220, 290, 632, 338, 262, 3878, 36724, 11, 11526, 4373, 11, 220, 1839, 393, 547, 19332, 329, 33953, 15434, 13, 383, 9912, 38102, 3520, 2968, 290, 389, 3058, 7025, 319, 9738, 287, 262, 471, 13, 50, 13, 1141, 262, 11188, 7028, 13, 383, 2631, 37555, 8663, 1138, 21684, 287, 21421, 11, 351, 262, 3800, 10530, 921, 821, 257, 4599, 1869, 11, 11526, 4373, 852, 257, 4388, 290, 1690, 12, 525, 12214, 3227, 13, 198, 198, 818, 2211, 11, 3195, 10005, 10307, 262, 2631, 37555, 5581, 38102, 262, 5544, 33575, 3195, 46838, 286, 1439, 3862, 13, 220, 317, 3644, 12, 11227, 515, 3895, 2646, 1912, 319, 262, 10283, 11, 383, 2631, 37555, 15875, 11, 373, 2716, 319, 3389, 718, 11, 1853, 13, 198, 198, 18122, 198, 198, 1129, 1821, 82, 198, 198, 6435, 37555, 550, 663, 8159, 287, 7455, 6, 75, 30938, 591, 11, 257, 10273, 6103, 9048, 326, 4120, 287, 3059, 37314, 338, 20994, 3348, 11, 262, 520, 13, 3362, 31437, 4332, 11, 422, 21709, 284, 11445, 13, 679, 717, 973, 262, 1438, 11526, 4373, 329, 257, 2095, 612, 11, 3584, 339, 5625, 262, 1438, 287, 1440, 308, 3775, 284, 1115, 1180, 6510, 290, 530, 11694, 287, 6450, 13, 383, 2168, 635, 550, 257, 3290, 326, 3114, 881, 588, 262, 1903, 11445, 82, 2196, 286, 42578, 11081, 13, 220, 554, 21794, 11, 3059, 37314, 2702, 257, 16251, 284, 383, 3909, 31867, 2947, 543, 3199, 1596, 2060, 12, 35330, 30070, 416, 3059, 37314, 13, 383, 717, 286, 777, 373, 286, 257, 2933, 5586, 351, 465, 3625, 319, 281, 267, 926, 5185, 13, 198, 198, 818, 21794, 11, 3059, 37314, 3088, 284, 423, 7455, 6, 75, 30938, 591, 11150, 3474, 832, 262, 49598, 14973, 5396, 11, 257, 4081, 1057, 416, 262, 1446, 14602, 82, 12, 32434, 7533, 6333, 13, 3059, 37314, 561, 423, 587, 281, 4795, 17195, 329, 262, 11150, 5344, 11, 36511, 286, 287, 262, 16236, 82, 11, 475, 262, 1730, 3214, 832, 13, 7455, 6, 75, 30938, 591, 373, 5710, 287, 1903, 11445, 13, 11450, 326, 614, 11, 3059, 37314, 10448, 262, 1578, 27018, 42271, 532, 635, 12228, 416, 1446, 14602, 82, 12, 32434, 532, 351, 465, 1266, 670, 422, 7455, 6, 75, 30938, 591, 13, 1649, 465, 670, 373, 6497, 510, 416, 1578, 27018, 42271, 11, 484, 3066, 284, 1057, 262, 649, 9048, 10283, 339, 550, 587, 1762, 319, 13, 770, 10283, 373, 2092, 287, 4437, 284, 262, 6103, 9048, 11, 475, 340, 550, 257, 900, 3350, 286, 3435, 11, 2138, 621, 1180, 299, 39942, 1310, 15504, 329, 1123, 2443, 13, 383, 1438, 7455, 6, 75, 30938, 591, 373, 1165, 1969, 284, 262, 3891, 286, 734, 584, 12770, 286, 262, 640, 25, 978, 327, 1324, 338, 7455, 6, 75, 2275, 1008, 290, 257, 10283, 11946, 7703, 30938, 591, 13, 1675, 3368, 10802, 11, 262, 11150, 5344, 10282, 319, 262, 1438, 2631, 37555, 11, 706, 262, 26636, 15604, 8096, 287, 262, 1374, 9892, 360, 702, 88, 3195, 905, 13, 220, 198, 198, 6435, 37555, 373, 257, 3670, 3059, 37314, 1464, 43252, 13, 554, 257, 12923, 2720, 11, 3059, 37314, 531, 286, 262, 3670, 2631, 37555, 25, 366, 1026, 338, 6635, 11441, 11, 468, 645, 3616, 11, 318, 2391, 15337, 11, 290, 468, 645, 16247, 960, 392, 314, 892, 616, 14733, 468, 16247, 526, 220, 383, 27458, 17268, 286, 262, 22670, 287, 46771, 1492, 1296, 6032, 550, 2035, 366, 37136, 4373, 1, 393, 366, 50, 3919, 11081, 1, 287, 262, 3670, 11, 407, 366, 6435, 37555, 1600, 780, 286, 3059, 37314, 6, 1233, 4594, 329, 465, 10283, 338, 3670, 13, 3574, 3389, 1160, 11, 19322, 284, 3269, 604, 11, 12923, 11, 262, 4756, 3502, 13043, 6032, 1100, 2631, 37555, 11, 9593, 4599, 6544, 6, 11526, 4373, 13, 198, 198, 42751, 82, 198, 198, 6435, 37555, 44119, 319, 3267, 362, 11, 11445, 11, 287, 5193, 14741, 25, 383, 2669, 2947, 11, 383, 4842, 17588, 11, 383, 20455, 17588, 11, 383, 1439, 298, 593, 14410, 4889, 11, 383, 46930, 18260, 12, 28595, 11, 383, 10656, 2947, 11, 383, 7312, 3782, 11, 383, 968, 1971, 2159, 12, 6767, 30536, 1222, 3825, 11, 290, 383, 6182, 18260, 13, 632, 2540, 355, 257, 4445, 10283, 13, 383, 717, 10283, 373, 1440, 13043, 890, 290, 3751, 11526, 4373, 6155, 416, 734, 584, 1862, 1751, 11, 6528, 1820, 290], [6385, 39923, 11, 262, 20715, 15895, 287, 33818, 7499, 468, 587, 11343, 13844, 284, 281, 1772, 422, 597, 1499, 508, 468, 11, 287, 262, 2456, 286, 262, 481, 286, 22044, 20715, 11, 4635, 366, 259, 262, 2214, 286, 9285, 262, 749, 11660, 670, 287, 281, 7306, 4571, 1, 357, 14986, 14023, 25, 2853, 3870, 287, 296, 25359, 1300, 77, 3971, 9920, 265, 1062, 285, 395, 5346, 301, 29090, 38396, 3326, 7126, 1312, 551, 7306, 1984, 374, 1134, 83, 768, 737, 7486, 1981, 2499, 389, 3360, 9181, 355, 852, 3573, 30902, 11, 994, 366, 1818, 1, 10229, 284, 281, 1772, 338, 670, 355, 257, 2187, 13, 383, 14023, 8581, 13267, 508, 11, 611, 2687, 11, 481, 3328, 262, 11596, 287, 597, 1813, 614, 13, 383, 21531, 26459, 262, 1438, 286, 262, 7147, 49734, 287, 1903, 3267, 13, 632, 318, 530, 286, 262, 1936, 20715, 4389, 12271, 4920, 416, 262, 481, 286, 22044, 20715, 287, 46425, 26, 262, 1854, 389, 262, 20715, 15895, 287, 27867, 11, 20715, 15895, 287, 23123, 11, 20715, 12689, 15895, 11, 290, 20715, 15895, 287, 8687, 12371, 393, 11558, 13, 198, 198, 21191, 417, 338, 3572, 286, 12476, 319, 7306, 1042, 287, 465, 9987, 329, 262, 20715, 15895, 287, 33818, 468, 2957, 284, 42465, 10386, 13, 554, 262, 2656, 14023, 11, 262, 1573, 7306, 1984, 23677, 355, 2035, 366, 485, 282, 2569, 1, 393, 366, 485, 282, 1911, 554, 262, 1903, 29112, 4289, 11, 262, 20715, 4606, 16173, 262, 6824, 286, 262, 481, 14084, 13, 1114, 428, 1738, 11, 484, 750, 407, 5764, 1728, 995, 12, 918, 11990, 7035, 286, 262, 640, 884, 355, 3700, 25936, 11, 19632, 20054, 301, 726, 11, 9261, 2580, 14636, 709, 11, 36547, 350, 472, 301, 11, 6752, 12602, 314, 1443, 268, 11, 290, 8616, 3700, 13, 3125, 2904, 11, 262, 26814, 468, 587, 517, 3655, 453, 16173, 13, 6660, 11, 262, 11596, 318, 783, 11343, 1111, 329, 15727, 16716, 17004, 290, 329, 2370, 286, 6414, 7306, 1042, 319, 617, 2383, 1241, 13, 554, 2274, 812, 11, 428, 1724, 257, 1611, 286, 7306, 1042, 8783, 278, 1692, 2489, 319, 257, 3154, 5046, 13, 16227, 11, 262, 5764, 318, 783, 15242, 517, 1964, 13, 198, 198, 464, 14023, 8581, 468, 12725, 2383, 7734, 287, 2274, 812, 329, 663, 9041, 286, 262, 5764, 13, 2773, 9188, 24351, 326, 867, 880, 12, 4002, 8786, 423, 407, 587, 11343, 262, 11596, 393, 772, 587, 19332, 290, 1854, 24351, 326, 617, 880, 12, 4002, 20352, 466, 407, 10925, 340, 13, 1318, 423, 635, 587, 36512, 7411, 4260, 1964, 5353, 11270, 284, 262, 11872, 1429, 290, 8713, 6356, 286, 617, 286, 262, 2274, 16716, 34515, 689, 13, 2773, 11, 884, 355, 3942, 8233, 9910, 533, 68, 11707, 430, 11, 423, 4367, 326, 11, 996, 262, 20715, 15895, 287, 33818, 318, 2383, 290, 12444, 284, 35562, 584, 13304, 11, 340, 318, 366, 1662, 262, 691, 18335, 286, 16716, 25230, 1911, 220, 198, 198, 21756, 220, 198, 198, 32, 1652, 445, 20715, 22111, 4817, 287, 465, 938, 481, 290, 29210, 326, 465, 1637, 307, 973, 284, 2251, 257, 2168, 286, 21740, 329, 883, 508, 35132, 262, 366, 18223, 395, 4414, 319, 18019, 1, 287, 11887, 11, 16585, 11, 4167, 11, 38033, 393, 9007, 11, 290, 9285, 13, 220, 220, 7486, 20715, 2630, 1811, 49928, 1141, 465, 10869, 11, 262, 938, 373, 3194, 257, 1310, 625, 257, 614, 878, 339, 3724, 11, 290, 4488, 379, 262, 14023, 12, 21991, 20684, 6289, 287, 6342, 319, 2681, 3389, 46425, 13, 220, 220, 20715, 307, 421, 4098, 704, 10048, 4, 286, 465, 2472, 6798, 11, 3261, 1849, 14100, 14023, 479, 1313, 273, 357, 2937, 3, 25096, 1510, 11, 10432, 17059, 1510, 287, 3648, 828, 284, 4474, 290, 886, 322, 262, 1936, 20715, 4389, 12271, 3693, 4023, 1378, 34952, 417, 3448, 2736, 13, 2398, 14, 1604, 445, 62, 34952, 417, 14, 10594, 14, 9630, 13, 6494, 366, 464, 2561, 286, 22044, 20715, 33116, 645, 6667, 3448, 2736, 13, 2398, 13, 43024, 718, 3389, 4343, 13, 14444, 284, 262, 1241, 286, 30186, 11965, 7346, 262, 481, 11, 340, 373, 407, 1566, 3035, 2608, 11, 49429, 326, 262, 520, 24707, 357, 21991, 20684, 8411, 8, 6325, 340, 13, 220, 220, 383, 3121, 669, 286, 465, 481, 547, 32797, 311, 48988, 805, 290, 17421, 4024, 406, 359, 73, 4853, 396, 11, 508, 7042, 262, 20715, 5693, 284, 1011, 1337, 286, 20715, 338, 15807, 290, 16481, 262, 21740, 13, 198, 198, 464, 1866, 286, 262, 22158, 20715, 4606, 326, 547, 284, 5764, 262, 12689, 15895, 547, 9899, 8972, 706, 262, 481, 373, 6325, 13, 383, 11596, 12, 707, 13493, 16435, 3940, 25, 262, 9375, 349, 1040, 4914, 37931, 315, 316, 319, 2795, 767, 11, 262, 14023, 8581, 319, 2795, 860, 11, 290, 262, 8111, 14023, 8581, 286, 13473, 319, 2795, 1367, 13, 220, 220, 383, 20715, 5693, 788, 4251, 281, 4381, 319, 9949, 329, 703, 262, 20715, 15895, 815, 307, 11343, 13, 554, 21489, 11, 262, 20715, 5693, 338, 8308, 2727, 24895, 547, 44532, 515, 416, 2677, 15694, 2873, 13, 685, 4023, 1378, 2503, 13, 65, 799, 1236, 3970, 13, 785, 14, 30195, 26752, 14, 26652, 14, 19, 14656, 3980, 14, 21191, 417, 12, 34487, 2736, 366, 21191, 417, 15895, 30866, 357, 12726, 828, 287, 2039, 22873, 21241, 67, 544, 46693, 3970, 11, 17535, 1315, 3269, 3717, 11, 422, 2039, 22873, 21241, 67, 544, 46693, 3970, 7467, 25, 220, 4784, 284, 20715, 338, 481, 11, 262, 8111, 14023, 8581, 547, 284, 5764, 262, 15895, 287, 33818, 13, 198, 198, 45, 27744, 8771, 198, 198, 10871, 614, 11, 262, 14023, 8581, 12800, 503, 7007, 329, 30994, 286, 5871, 329, 262, 20715, 15895, 287, 33818, 13, 12688, 286, 262, 8581, 11, 1866, 286, 9285, 5790, 5090, 290, 14515, 11, 20339, 286, 9285, 290, 3303, 11, 1966, 20715, 9285, 34515, 689, 11, 290, 262, 19033, 286, 8786, 6, 5745, 389, 477, 3142, 284, 40376, 257, 4540, 13, 632, 318, 407, 10431, 284, 40376, 27186, 13, 198, 198, 37482, 286, 7007, 389, 1908, 503, 1123, 614, 11, 290, 220, 546, 15629, 11628, 389, 4504, 13, 5990, 370, 11033, 301, 3900, 357, 10364, 286, 383, 20715, 4606, 329, 33818, 828, 685, 4023, 1378, 2503, 13, 3281, 12106, 13, 785]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(context[0:2], max_length=1024, truncation=True, return_token_type_ids=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    wiki_contexts = [source['wiki_context'][0].strip() if len(source['wiki_context']) > 0 else 'no context'.strip() for source in examples['entity_pages'] ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        wiki_contexts,\n",
    "        max_length=1024,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    # answers = examples[\"answers\"]\n",
    "    # start_positions = []\n",
    "    # end_positions = []\n",
    "\n",
    "    # for i, offset in enumerate(offset_mapping):\n",
    "    #     # answer = answers[i]\n",
    "    #     # start_char = answer[\"answer_start\"][0]\n",
    "    #     # end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    #     sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    #     # Find the start and end of the context\n",
    "    #     idx = 0\n",
    "    #     while sequence_ids[idx] != 1:\n",
    "    #         idx += 1\n",
    "    #     context_start = idx\n",
    "\n",
    "    #     while sequence_ids[idx] == 1:\n",
    "    #         idx += 1\n",
    "    #     context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "    #     if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "    #         start_positions.append(0)\n",
    "    #         end_positions.append(0)\n",
    "    #     else:\n",
    "    #         # Otherwise it's the start and end token positions\n",
    "    #         idx = context_start\n",
    "    #         while idx <= context_end and offset[idx][0] <= start_char:\n",
    "    #             idx += 1\n",
    "    #         start_positions.append(idx - 1)\n",
    "\n",
    "    #         idx = context_end\n",
    "    #         while idx >= context_start and offset[idx][1] >= end_char:\n",
    "    #             idx -= 1\n",
    "    #         end_positions.append(idx + 1)\n",
    "\n",
    "    # inputs[\"start_positions\"] = start_positions\n",
    "    # inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wonseok/.cache/huggingface/datasets/json/default-62b5c3c6749fb4c0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8786452bd4631e7.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0d0206d8b3427baea4a2648c67d503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_test = test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e385efa8c64f498c9267db005939ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/trivia_gpt\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['train'][0]['entity_pages']['wiki_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt', return_token_type_ids=True, max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-3.1998, -9.9643, -8.0330,  ..., -6.3425, -5.7444, -6.0709]]), end_logits=tensor([[-0.0966, -0.7796, -0.9979,  ..., -1.6981, -1.8853, -1.9204]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3874,  6.4358,  5.6925,  5.6773,  5.6681,  5.2134,  5.1008,  5.8855,\n",
       "          5.3893,  5.8450,  5.3206,  5.4511,  5.6941,  5.4772,  5.1780,  5.5038,\n",
       "          5.4234,  4.6833,  4.7428,  4.3512,  5.7624,  4.6304,  3.8184,  3.7696,\n",
       "          4.0553,  3.6392,  4.7540,  4.3875,  3.5800,  3.8620,  3.5720,  3.4407,\n",
       "          3.1645,  3.3903,  4.7443,  5.5327,  4.2520,  3.7863,  2.0929,  1.4395,\n",
       "          4.9716,  3.3676,  3.4566,  3.7941,  4.2121,  2.7184,  1.9635,  1.9507,\n",
       "          4.4011,  2.8790,  2.5880,  2.7406,  0.9453,  3.2911,  2.6771,  3.8936,\n",
       "          5.1057,  2.8449,  4.4624,  3.8717,  4.5720,  4.3178,  5.1858,  4.0228,\n",
       "          3.8016,  4.3358,  4.6010,  4.0006,  3.4059,  4.0620,  3.5027,  4.5354,\n",
       "          4.6953,  5.2006,  5.0762,  3.5061,  4.3643,  4.9156,  4.7615,  4.4697,\n",
       "          4.1841,  3.9989,  4.2213,  3.8873,  4.2942,  2.3721,  4.0536,  3.2292,\n",
       "          3.0520,  3.4255,  2.8044,  3.4301,  4.2049,  3.5849,  3.4642,  4.4165,\n",
       "          5.4953,  5.9342,  6.3901,  5.3128,  5.4550,  2.0939,  5.4535,  4.0030,\n",
       "          4.5778,  2.3536,  2.9545,  4.9644,  4.6041,  4.3900,  3.8844,  4.5457,\n",
       "          4.4744,  3.4893,  5.0479,  3.3361,  4.2717,  4.9955,  3.8155,  3.1670,\n",
       "          3.1498,  4.4667,  5.1573,  4.3524,  3.8490,  2.6661,  3.3388,  4.6812,\n",
       "          4.5827,  4.2959,  3.0814,  4.6747,  3.4173,  4.5826,  4.8345,  4.7922,\n",
       "          4.6837,  4.1886,  3.0895,  3.0139,  4.6679,  3.7320,  3.3472,  3.5082,\n",
       "          3.7691,  2.3673,  3.7543,  3.4093,  3.3285,  3.4496,  3.5137,  3.8134,\n",
       "          2.7601,  3.1236,  3.1522,  1.0074,  4.3315,  4.3876,  3.4904,  3.2448,\n",
       "          3.7531,  3.2741,  3.7476,  2.0597,  3.3866,  4.4775,  4.2109,  4.1440,\n",
       "          4.1498,  3.0966,  3.9041,  2.7999,  4.1050,  3.8917,  2.5531,  3.5567,\n",
       "          3.7825,  3.7885,  2.9593,  2.2612,  2.9095,  5.0659,  3.7918,  3.0946,\n",
       "          4.1123,  2.7681,  2.8363,  3.3575,  3.4929,  1.4093,  2.0421,  3.2170,\n",
       "          3.0153,  4.0884,  3.9686,  3.5551,  4.1189,  3.8032,  3.4748,  4.4662,\n",
       "          4.2970,  4.1206,  3.5956,  3.9710,  8.3686,  3.1941,  4.0349,  6.3627,\n",
       "          2.7639,  3.3787,  2.8024,  2.3525,  3.3750,  2.7467,  3.0546,  3.2599,\n",
       "          2.7994,  2.5306,  2.9498,  2.7526,  3.2985,  2.7180,  3.0015,  3.2053,\n",
       "          2.3164,  2.5894,  3.3014,  2.4419,  3.5989,  2.2524,  4.5448,  4.8005,\n",
       "          4.5457,  4.1868,  5.3550,  4.4280,  2.7179,  3.1648,  3.2651,  3.6734,\n",
       "          1.5185,  3.3919,  0.6451,  3.1730,  3.0643,  3.1575,  2.7518,  3.5169,\n",
       "          2.6440,  2.4985,  3.0426,  2.5787,  2.7342,  3.8752,  2.4080,  3.3240,\n",
       "          1.9823,  1.0627,  2.5033,  3.4280,  3.7623,  2.0479,  3.4044,  1.4075,\n",
       "          2.8168,  1.6496,  1.5042,  2.8894,  3.4949,  3.0971,  1.8820,  2.3078,\n",
       "          1.7852,  2.7267,  0.3885,  2.8461,  2.6310,  3.7389,  4.1455,  3.3437,\n",
       "          2.5272,  2.8974,  2.9576,  2.4670,  2.4896,  1.1713,  0.4298,  1.5355,\n",
       "          2.2496,  1.7222,  0.9783,  1.9836,  0.7649,  2.0906, -0.0944,  3.0246,\n",
       "          1.9539,  2.4284,  2.2084,  2.2147,  2.3104,  2.3297,  1.4931,  1.9801,\n",
       "          0.6251,  4.0742,  2.5692, -0.1389, -0.8585,  0.6539,  2.6855,  4.0566,\n",
       "          3.6931,  4.5006,  2.5635,  2.2488,  2.1858,  2.5210,  2.3982,  2.0569,\n",
       "          3.2348,  2.3490,  2.1087,  2.5530,  2.9663,  2.5363, 11.2124,  2.5743,\n",
       "          2.0226,  2.6285,  0.8461, 10.0033,  2.6272,  2.8977,  2.2049,  3.4175,\n",
       "         -0.7508,  0.8820,  0.7207, -1.0885,  0.1359,  2.5683,  1.7592,  1.1912,\n",
       "          1.5574,  0.4452,  2.8252,  2.3998,  2.9427,  3.1565,  2.6291,  1.8402,\n",
       "          2.8109,  2.8712,  1.7253,  2.6360,  2.2020,  1.4156,  2.5642,  2.1530,\n",
       "          0.1402,  2.4002,  0.7471,  0.4674,  0.9221,  1.0159,  1.7390,  4.0620,\n",
       "          1.5659,  2.7247,  2.6061,  2.5161,  2.0861, -0.3445,  1.5178,  0.4561,\n",
       "          3.0025,  3.4950,  2.5513,  3.5404,  1.6439,  1.5852,  2.2517,  2.2903,\n",
       "          2.0871,  0.0784,  2.1838,  2.8659,  2.9423,  3.6587,  4.8702,  5.0702,\n",
       "          3.3484,  1.5988,  2.8877,  2.3649,  2.5302,  1.4300, -0.5739,  2.6603,\n",
       "          1.9136,  3.3246,  4.0956,  2.8359,  3.3274,  2.9821,  1.9973,  2.1676,\n",
       "          3.3961,  3.0325,  2.9494,  2.6711,  3.1380,  4.3494,  4.0253,  2.2151,\n",
       "          0.2351,  0.6742,  0.2975,  2.4540,  1.9885,  7.2125,  1.7328,  4.4303,\n",
       "          4.4398,  3.8244,  2.3886,  3.3733,  3.5084,  4.4643,  3.0732,  2.2262,\n",
       "          0.6566, -0.8891,  2.3101,  3.4261,  2.8782,  1.9156,  3.2155, -2.0225,\n",
       "          3.9290,  0.7209,  3.3363,  3.6737,  3.1841,  1.1686,  2.1089,  1.3211,\n",
       "          0.3193,  2.3969,  2.7655,  1.9875,  1.4447,  1.0721,  1.0851,  1.1607,\n",
       "          1.8804,  3.0394,  8.2931,  3.3064,  2.3189,  3.9248,  1.9727,  1.4525,\n",
       "          4.8007,  3.4775,  3.5329,  4.3161,  4.1687,  2.7522,  4.7119,  3.9023,\n",
       "          3.6283,  3.3390,  2.8574,  2.7255,  2.8955,  3.0887,  3.2817,  3.7472,\n",
       "          3.3166,  2.1993,  3.2001,  2.5999,  2.7795,  2.5175,  2.3882,  0.8443,\n",
       "          0.4935,  3.0557,  4.1618,  3.0802,  2.4087,  4.0307,  3.8913,  2.7009,\n",
       "          2.9910,  0.9062, -1.0440,  3.4777,  3.5121,  3.4768,  3.1996,  3.6195,\n",
       "          2.0298,  2.7009,  3.3946,  3.0067,  2.4879,  2.8226,  3.2677,  3.6154,\n",
       "         11.3284,  3.0015,  4.2927,  3.6511,  2.7463,  2.2718,  4.3450,  4.3268,\n",
       "          4.0147,  2.7982,  1.0192,  4.0189,  3.6694,  3.6793,  3.9586,  4.8148,\n",
       "          5.0763,  3.9070,  3.9744,  3.8095,  4.0304,  1.5355,  3.4693,  3.1564,\n",
       "          3.3475,  4.0869,  3.2545,  3.7089,  3.8482,  3.5890,  3.1607,  3.1287,\n",
       "          2.1103,  3.5223, -0.2475,  3.8965,  3.5294,  2.4097,  3.7758,  3.3937,\n",
       "          4.3487,  3.7437,  3.2806,  3.1613,  3.6670,  3.0927,  4.6408, -0.4208,\n",
       "          2.7881,  2.1833,  8.0682,  2.9055,  4.7515,  4.2171,  3.8451,  4.6064,\n",
       "          1.8838,  4.7419,  4.2775,  2.2470,  4.3865,  3.5642,  2.3720, -2.0127,\n",
       "          2.3417,  1.1976,  3.0215,  2.9936,  1.4902,  1.9293,  2.5053,  4.0947,\n",
       "          3.2800,  3.0633,  2.5422,  2.5209,  1.5736,  1.6298,  3.0748,  3.4313,\n",
       "          1.9133,  2.4304,  2.8032,  2.2758,  3.6665,  3.1900,  4.6024,  3.7900,\n",
       "          2.9771,  7.5620,  2.2896,  3.7443,  3.7208,  3.1850, 11.2411,  2.7717,\n",
       "          3.7872,  4.2009,  3.5447, 11.4206,  3.5130,  4.4753,  3.8460,  3.6173,\n",
       "          9.1210,  3.7112,  2.4387,  3.5351,  2.8179, 11.4855,  3.3726,  4.2093,\n",
       "          3.5493,  4.1930,  3.3004,  3.8147,  3.3996,  3.4463,  3.7994,  4.2143,\n",
       "          4.1470,  3.5165,  1.7377,  0.1211, -0.3203,  1.5391,  3.5876,  2.6416,\n",
       "          1.8468,  4.4094,  4.5793,  3.2261,  3.7346,  3.4702,  1.6205,  4.6328,\n",
       "          3.1221,  4.6808,  4.0559,  4.0391,  4.0348, -1.7247,  2.3344,  1.6798,\n",
       "          3.3944,  3.4179,  3.3800,  4.1287,  2.5609,  3.8105,  3.8659,  1.9612,\n",
       "          3.5341,  1.2720,  2.6469,  2.3937,  1.8874,  3.6586,  3.4099,  3.0412,\n",
       "          4.1409,  2.8883,  3.6501,  3.7058,  2.8210,  1.0612,  8.6747,  3.3096,\n",
       "          3.4657,  3.1896,  2.6655, 11.5181,  2.9018,  3.6897,  2.5662,  2.5409,\n",
       "          1.9945,  2.2508,  1.7645,  2.1165,  2.7855,  3.1657,  3.4107,  3.7274,\n",
       "          3.9761,  1.6600,  1.4884, -0.0664,  2.4888,  3.1281,  3.4525,  3.7688,\n",
       "          2.9261,  3.3236,  4.0429,  2.4772,  1.9641,  2.1838,  0.8781,  2.4074,\n",
       "          0.1911,  1.1852,  1.3542,  1.0972,  2.0290,  0.6596,  2.9239,  1.9129,\n",
       "         -0.8697, -0.8327,  1.5276, -1.4199, -0.9840,  0.9272,  1.7942,  2.2079,\n",
       "          0.0898,  0.8235,  1.1458, -1.7473,  2.9147,  3.1152,  2.8763, -0.2875,\n",
       "          0.0137, -0.0806, -1.9001, -0.1102,  1.7573,  0.3755,  2.2426,  1.7579,\n",
       "          1.5980,  0.6145,  1.4763,  1.1791, -0.9108,  1.1974, -0.0593, -0.7414,\n",
       "         -3.5253,  0.1608, -1.1813, -0.9518,  1.0356,  1.1581, -0.6900,  0.2785,\n",
       "          0.6377, -1.0936,  1.7556,  2.0700,  0.4042,  0.4366,  1.2198, -0.5415,\n",
       "          0.3600,  1.1101,  0.2500,  8.8758,  2.8886,  3.3934,  3.3132,  6.2888,\n",
       "          0.8099,  0.9155,  2.3770,  0.3503,  2.0829,  3.7771,  3.7092,  2.5752,\n",
       "          3.4898,  3.5323,  2.7030,  2.1569,  2.1585,  1.0412, -1.6823,  2.5162,\n",
       "          2.1062,  0.3667,  3.5072,  3.5104,  0.8129, -0.9410, -3.7120, -0.2709,\n",
       "         10.5728,  1.9947,  4.2726,  3.1574,  2.1623, 11.3986,  2.5104,  2.1910,\n",
       "          3.2871,  4.2724,  3.1546,  3.8044,  2.9897,  3.0143,  2.6704, 11.4923,\n",
       "          3.1722,  3.2759,  2.7858, -1.1551,  0.6035, -0.1749,  0.3931,  1.3926,\n",
       "          3.8628, 11.4952,  2.8096,  3.7630,  3.6157, 10.9599,  2.9419,  3.7709,\n",
       "          3.5268,  3.1428,  3.8732,  3.1690,  4.0209,  3.3221,  2.6565,  2.6784,\n",
       "         10.1398,  3.3049,  4.5611,  3.2225,  2.5767,  4.1549,  4.2925,  3.3014,\n",
       "          8.9031,  3.6697,  4.2935,  4.5584,  3.4275,  3.0732,  8.4472,  3.5039,\n",
       "          4.0415,  3.5188,  3.4671,  1.8325,  1.8449,  8.6792,  3.3881,  3.9743,\n",
       "          2.9745,  3.0469,  9.7741,  3.3421,  4.6183,  3.0195,  3.2072,  3.1167,\n",
       "          3.5168,  2.6709,  2.6758, -1.0266,  3.6984,  3.6273,  3.7156,  3.9445,\n",
       "          9.2836,  2.9226,  4.4337,  4.4430,  3.7348,  2.0298,  2.0770,  2.1049,\n",
       "          8.2502,  3.0935,  4.2710,  3.6928,  7.0115,  4.2054,  4.6002,  3.1715,\n",
       "          4.3150,  3.5727,  3.3193,  3.4133,  2.4982,  2.6346,  3.9701,  3.3702,\n",
       "          3.9843,  4.1175,  2.3332]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
