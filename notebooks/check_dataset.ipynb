{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonseok/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-18 19:49:03.499843: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-18 19:49:03.598801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-18 19:49:03.598924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-18 19:49:03.616004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-18 19:49:03.654896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-18 19:49:04.323785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from pathlib import Path\n",
    "from transformers import GPT2Tokenizer, GPT2Model,  AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "project_dir = Path().cwd().parent\n",
    "data_dir = project_dir.joinpath(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/processed/triviaqa_train.json','r') as f :\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(paragraph) :\n",
    "    return paragraph['paragraphs'][0]\n",
    "\n",
    "all_data = [extractor(book) for book in data['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/trivia.json', 'w', encoding='utf-8') as f :\n",
    "    json.dump(all_data, f)\n",
    "# data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/wonseok/.cache/huggingface/datasets/json/default-9251c168c2e7bf69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=\"../data/processed/trivia.json\", split='train[:10000]')\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [],\n",
       " 'id': 'tc_1062--Chemical_element.txt',\n",
       " 'is_impossible': True,\n",
       " 'question': 'Which element is named after Pierre and Marie Curie?'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['qas'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q[0]['question'].strip() for q in examples[\"qas\"]]\n",
    "    contexts = [context.strip() for context in examples['context']]\n",
    "    answers = [q[0]['answers'] for q in examples[\"qas\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=1024,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        if len(answers[i]) < 1 :\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "        else :\n",
    "            answer = answers[i][0]\n",
    "\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309e580f85384cedbeff7d0bfcfa61e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/wonseok/Insync/gememy85@gmail.com/Google Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb ì…€ 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenized_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(preprocess_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3463\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3459\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3460\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3461\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3463\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3464\u001b[0m         batch,\n\u001b[1;32m   3465\u001b[0m         indices,\n\u001b[1;32m   3466\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3467\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3468\u001b[0m     )\n\u001b[1;32m   3469\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3470\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3471\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3472\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "\u001b[1;32m/home/wonseok/Insync/gememy85@gmail.com/Google Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb ì…€ 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m end_positions \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, offset \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(offset_mapping):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     answer \u001b[39m=\u001b[39m answers[i][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     start_char \u001b[39m=\u001b[39m answer[\u001b[39m\"\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wonseok/Insync/gememy85%40gmail.com/Google%20Drive/projects/2023_nlp_project/notebooks/check_dataset.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     end_char \u001b[39m=\u001b[39m answer[\u001b[39m\"\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(answer[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e385efa8c64f498c9267db005939ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/trivia_gpt\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['train'][0]['entity_pages']['wiki_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt', return_token_type_ids=True, max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-3.1998, -9.9643, -8.0330,  ..., -6.3425, -5.7444, -6.0709]]), end_logits=tensor([[-0.0966, -0.7796, -0.9979,  ..., -1.6981, -1.8853, -1.9204]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3874,  6.4358,  5.6925,  5.6773,  5.6681,  5.2134,  5.1008,  5.8855,\n",
       "          5.3893,  5.8450,  5.3206,  5.4511,  5.6941,  5.4772,  5.1780,  5.5038,\n",
       "          5.4234,  4.6833,  4.7428,  4.3512,  5.7624,  4.6304,  3.8184,  3.7696,\n",
       "          4.0553,  3.6392,  4.7540,  4.3875,  3.5800,  3.8620,  3.5720,  3.4407,\n",
       "          3.1645,  3.3903,  4.7443,  5.5327,  4.2520,  3.7863,  2.0929,  1.4395,\n",
       "          4.9716,  3.3676,  3.4566,  3.7941,  4.2121,  2.7184,  1.9635,  1.9507,\n",
       "          4.4011,  2.8790,  2.5880,  2.7406,  0.9453,  3.2911,  2.6771,  3.8936,\n",
       "          5.1057,  2.8449,  4.4624,  3.8717,  4.5720,  4.3178,  5.1858,  4.0228,\n",
       "          3.8016,  4.3358,  4.6010,  4.0006,  3.4059,  4.0620,  3.5027,  4.5354,\n",
       "          4.6953,  5.2006,  5.0762,  3.5061,  4.3643,  4.9156,  4.7615,  4.4697,\n",
       "          4.1841,  3.9989,  4.2213,  3.8873,  4.2942,  2.3721,  4.0536,  3.2292,\n",
       "          3.0520,  3.4255,  2.8044,  3.4301,  4.2049,  3.5849,  3.4642,  4.4165,\n",
       "          5.4953,  5.9342,  6.3901,  5.3128,  5.4550,  2.0939,  5.4535,  4.0030,\n",
       "          4.5778,  2.3536,  2.9545,  4.9644,  4.6041,  4.3900,  3.8844,  4.5457,\n",
       "          4.4744,  3.4893,  5.0479,  3.3361,  4.2717,  4.9955,  3.8155,  3.1670,\n",
       "          3.1498,  4.4667,  5.1573,  4.3524,  3.8490,  2.6661,  3.3388,  4.6812,\n",
       "          4.5827,  4.2959,  3.0814,  4.6747,  3.4173,  4.5826,  4.8345,  4.7922,\n",
       "          4.6837,  4.1886,  3.0895,  3.0139,  4.6679,  3.7320,  3.3472,  3.5082,\n",
       "          3.7691,  2.3673,  3.7543,  3.4093,  3.3285,  3.4496,  3.5137,  3.8134,\n",
       "          2.7601,  3.1236,  3.1522,  1.0074,  4.3315,  4.3876,  3.4904,  3.2448,\n",
       "          3.7531,  3.2741,  3.7476,  2.0597,  3.3866,  4.4775,  4.2109,  4.1440,\n",
       "          4.1498,  3.0966,  3.9041,  2.7999,  4.1050,  3.8917,  2.5531,  3.5567,\n",
       "          3.7825,  3.7885,  2.9593,  2.2612,  2.9095,  5.0659,  3.7918,  3.0946,\n",
       "          4.1123,  2.7681,  2.8363,  3.3575,  3.4929,  1.4093,  2.0421,  3.2170,\n",
       "          3.0153,  4.0884,  3.9686,  3.5551,  4.1189,  3.8032,  3.4748,  4.4662,\n",
       "          4.2970,  4.1206,  3.5956,  3.9710,  8.3686,  3.1941,  4.0349,  6.3627,\n",
       "          2.7639,  3.3787,  2.8024,  2.3525,  3.3750,  2.7467,  3.0546,  3.2599,\n",
       "          2.7994,  2.5306,  2.9498,  2.7526,  3.2985,  2.7180,  3.0015,  3.2053,\n",
       "          2.3164,  2.5894,  3.3014,  2.4419,  3.5989,  2.2524,  4.5448,  4.8005,\n",
       "          4.5457,  4.1868,  5.3550,  4.4280,  2.7179,  3.1648,  3.2651,  3.6734,\n",
       "          1.5185,  3.3919,  0.6451,  3.1730,  3.0643,  3.1575,  2.7518,  3.5169,\n",
       "          2.6440,  2.4985,  3.0426,  2.5787,  2.7342,  3.8752,  2.4080,  3.3240,\n",
       "          1.9823,  1.0627,  2.5033,  3.4280,  3.7623,  2.0479,  3.4044,  1.4075,\n",
       "          2.8168,  1.6496,  1.5042,  2.8894,  3.4949,  3.0971,  1.8820,  2.3078,\n",
       "          1.7852,  2.7267,  0.3885,  2.8461,  2.6310,  3.7389,  4.1455,  3.3437,\n",
       "          2.5272,  2.8974,  2.9576,  2.4670,  2.4896,  1.1713,  0.4298,  1.5355,\n",
       "          2.2496,  1.7222,  0.9783,  1.9836,  0.7649,  2.0906, -0.0944,  3.0246,\n",
       "          1.9539,  2.4284,  2.2084,  2.2147,  2.3104,  2.3297,  1.4931,  1.9801,\n",
       "          0.6251,  4.0742,  2.5692, -0.1389, -0.8585,  0.6539,  2.6855,  4.0566,\n",
       "          3.6931,  4.5006,  2.5635,  2.2488,  2.1858,  2.5210,  2.3982,  2.0569,\n",
       "          3.2348,  2.3490,  2.1087,  2.5530,  2.9663,  2.5363, 11.2124,  2.5743,\n",
       "          2.0226,  2.6285,  0.8461, 10.0033,  2.6272,  2.8977,  2.2049,  3.4175,\n",
       "         -0.7508,  0.8820,  0.7207, -1.0885,  0.1359,  2.5683,  1.7592,  1.1912,\n",
       "          1.5574,  0.4452,  2.8252,  2.3998,  2.9427,  3.1565,  2.6291,  1.8402,\n",
       "          2.8109,  2.8712,  1.7253,  2.6360,  2.2020,  1.4156,  2.5642,  2.1530,\n",
       "          0.1402,  2.4002,  0.7471,  0.4674,  0.9221,  1.0159,  1.7390,  4.0620,\n",
       "          1.5659,  2.7247,  2.6061,  2.5161,  2.0861, -0.3445,  1.5178,  0.4561,\n",
       "          3.0025,  3.4950,  2.5513,  3.5404,  1.6439,  1.5852,  2.2517,  2.2903,\n",
       "          2.0871,  0.0784,  2.1838,  2.8659,  2.9423,  3.6587,  4.8702,  5.0702,\n",
       "          3.3484,  1.5988,  2.8877,  2.3649,  2.5302,  1.4300, -0.5739,  2.6603,\n",
       "          1.9136,  3.3246,  4.0956,  2.8359,  3.3274,  2.9821,  1.9973,  2.1676,\n",
       "          3.3961,  3.0325,  2.9494,  2.6711,  3.1380,  4.3494,  4.0253,  2.2151,\n",
       "          0.2351,  0.6742,  0.2975,  2.4540,  1.9885,  7.2125,  1.7328,  4.4303,\n",
       "          4.4398,  3.8244,  2.3886,  3.3733,  3.5084,  4.4643,  3.0732,  2.2262,\n",
       "          0.6566, -0.8891,  2.3101,  3.4261,  2.8782,  1.9156,  3.2155, -2.0225,\n",
       "          3.9290,  0.7209,  3.3363,  3.6737,  3.1841,  1.1686,  2.1089,  1.3211,\n",
       "          0.3193,  2.3969,  2.7655,  1.9875,  1.4447,  1.0721,  1.0851,  1.1607,\n",
       "          1.8804,  3.0394,  8.2931,  3.3064,  2.3189,  3.9248,  1.9727,  1.4525,\n",
       "          4.8007,  3.4775,  3.5329,  4.3161,  4.1687,  2.7522,  4.7119,  3.9023,\n",
       "          3.6283,  3.3390,  2.8574,  2.7255,  2.8955,  3.0887,  3.2817,  3.7472,\n",
       "          3.3166,  2.1993,  3.2001,  2.5999,  2.7795,  2.5175,  2.3882,  0.8443,\n",
       "          0.4935,  3.0557,  4.1618,  3.0802,  2.4087,  4.0307,  3.8913,  2.7009,\n",
       "          2.9910,  0.9062, -1.0440,  3.4777,  3.5121,  3.4768,  3.1996,  3.6195,\n",
       "          2.0298,  2.7009,  3.3946,  3.0067,  2.4879,  2.8226,  3.2677,  3.6154,\n",
       "         11.3284,  3.0015,  4.2927,  3.6511,  2.7463,  2.2718,  4.3450,  4.3268,\n",
       "          4.0147,  2.7982,  1.0192,  4.0189,  3.6694,  3.6793,  3.9586,  4.8148,\n",
       "          5.0763,  3.9070,  3.9744,  3.8095,  4.0304,  1.5355,  3.4693,  3.1564,\n",
       "          3.3475,  4.0869,  3.2545,  3.7089,  3.8482,  3.5890,  3.1607,  3.1287,\n",
       "          2.1103,  3.5223, -0.2475,  3.8965,  3.5294,  2.4097,  3.7758,  3.3937,\n",
       "          4.3487,  3.7437,  3.2806,  3.1613,  3.6670,  3.0927,  4.6408, -0.4208,\n",
       "          2.7881,  2.1833,  8.0682,  2.9055,  4.7515,  4.2171,  3.8451,  4.6064,\n",
       "          1.8838,  4.7419,  4.2775,  2.2470,  4.3865,  3.5642,  2.3720, -2.0127,\n",
       "          2.3417,  1.1976,  3.0215,  2.9936,  1.4902,  1.9293,  2.5053,  4.0947,\n",
       "          3.2800,  3.0633,  2.5422,  2.5209,  1.5736,  1.6298,  3.0748,  3.4313,\n",
       "          1.9133,  2.4304,  2.8032,  2.2758,  3.6665,  3.1900,  4.6024,  3.7900,\n",
       "          2.9771,  7.5620,  2.2896,  3.7443,  3.7208,  3.1850, 11.2411,  2.7717,\n",
       "          3.7872,  4.2009,  3.5447, 11.4206,  3.5130,  4.4753,  3.8460,  3.6173,\n",
       "          9.1210,  3.7112,  2.4387,  3.5351,  2.8179, 11.4855,  3.3726,  4.2093,\n",
       "          3.5493,  4.1930,  3.3004,  3.8147,  3.3996,  3.4463,  3.7994,  4.2143,\n",
       "          4.1470,  3.5165,  1.7377,  0.1211, -0.3203,  1.5391,  3.5876,  2.6416,\n",
       "          1.8468,  4.4094,  4.5793,  3.2261,  3.7346,  3.4702,  1.6205,  4.6328,\n",
       "          3.1221,  4.6808,  4.0559,  4.0391,  4.0348, -1.7247,  2.3344,  1.6798,\n",
       "          3.3944,  3.4179,  3.3800,  4.1287,  2.5609,  3.8105,  3.8659,  1.9612,\n",
       "          3.5341,  1.2720,  2.6469,  2.3937,  1.8874,  3.6586,  3.4099,  3.0412,\n",
       "          4.1409,  2.8883,  3.6501,  3.7058,  2.8210,  1.0612,  8.6747,  3.3096,\n",
       "          3.4657,  3.1896,  2.6655, 11.5181,  2.9018,  3.6897,  2.5662,  2.5409,\n",
       "          1.9945,  2.2508,  1.7645,  2.1165,  2.7855,  3.1657,  3.4107,  3.7274,\n",
       "          3.9761,  1.6600,  1.4884, -0.0664,  2.4888,  3.1281,  3.4525,  3.7688,\n",
       "          2.9261,  3.3236,  4.0429,  2.4772,  1.9641,  2.1838,  0.8781,  2.4074,\n",
       "          0.1911,  1.1852,  1.3542,  1.0972,  2.0290,  0.6596,  2.9239,  1.9129,\n",
       "         -0.8697, -0.8327,  1.5276, -1.4199, -0.9840,  0.9272,  1.7942,  2.2079,\n",
       "          0.0898,  0.8235,  1.1458, -1.7473,  2.9147,  3.1152,  2.8763, -0.2875,\n",
       "          0.0137, -0.0806, -1.9001, -0.1102,  1.7573,  0.3755,  2.2426,  1.7579,\n",
       "          1.5980,  0.6145,  1.4763,  1.1791, -0.9108,  1.1974, -0.0593, -0.7414,\n",
       "         -3.5253,  0.1608, -1.1813, -0.9518,  1.0356,  1.1581, -0.6900,  0.2785,\n",
       "          0.6377, -1.0936,  1.7556,  2.0700,  0.4042,  0.4366,  1.2198, -0.5415,\n",
       "          0.3600,  1.1101,  0.2500,  8.8758,  2.8886,  3.3934,  3.3132,  6.2888,\n",
       "          0.8099,  0.9155,  2.3770,  0.3503,  2.0829,  3.7771,  3.7092,  2.5752,\n",
       "          3.4898,  3.5323,  2.7030,  2.1569,  2.1585,  1.0412, -1.6823,  2.5162,\n",
       "          2.1062,  0.3667,  3.5072,  3.5104,  0.8129, -0.9410, -3.7120, -0.2709,\n",
       "         10.5728,  1.9947,  4.2726,  3.1574,  2.1623, 11.3986,  2.5104,  2.1910,\n",
       "          3.2871,  4.2724,  3.1546,  3.8044,  2.9897,  3.0143,  2.6704, 11.4923,\n",
       "          3.1722,  3.2759,  2.7858, -1.1551,  0.6035, -0.1749,  0.3931,  1.3926,\n",
       "          3.8628, 11.4952,  2.8096,  3.7630,  3.6157, 10.9599,  2.9419,  3.7709,\n",
       "          3.5268,  3.1428,  3.8732,  3.1690,  4.0209,  3.3221,  2.6565,  2.6784,\n",
       "         10.1398,  3.3049,  4.5611,  3.2225,  2.5767,  4.1549,  4.2925,  3.3014,\n",
       "          8.9031,  3.6697,  4.2935,  4.5584,  3.4275,  3.0732,  8.4472,  3.5039,\n",
       "          4.0415,  3.5188,  3.4671,  1.8325,  1.8449,  8.6792,  3.3881,  3.9743,\n",
       "          2.9745,  3.0469,  9.7741,  3.3421,  4.6183,  3.0195,  3.2072,  3.1167,\n",
       "          3.5168,  2.6709,  2.6758, -1.0266,  3.6984,  3.6273,  3.7156,  3.9445,\n",
       "          9.2836,  2.9226,  4.4337,  4.4430,  3.7348,  2.0298,  2.0770,  2.1049,\n",
       "          8.2502,  3.0935,  4.2710,  3.6928,  7.0115,  4.2054,  4.6002,  3.1715,\n",
       "          4.3150,  3.5727,  3.3193,  3.4133,  2.4982,  2.6346,  3.9701,  3.3702,\n",
       "          3.9843,  4.1175,  2.3332]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
